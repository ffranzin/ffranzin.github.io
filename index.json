[{"categories":null,"contents":"","permalink":"/publications/a-raster-based-approach-for-waterbodies-mesh-generation/","tags":["Mesh Generation","Geographic Information System (GIS)","Large Scale scenarios","tool programming","optimization"],"title":"A Raster-based Approach for Waterbodies Mesh Generation"},{"categories":null,"contents":"","permalink":"/publications/accurate-real-time-physics-simulation-for-large-worlds/","tags":["Real-time Physics Simulation","Large scale scenarios","Floating-point Precision"],"title":"Accurate Real-time Physics Simulation for Large Worlds"},{"categories":null,"contents":"","permalink":"/publications/gpu-based-rendering-and-collision-simulation-of-ground-vegetation-in-large-scace-virtual-scenarios/","tags":["Ground Vegetation","Collision Simulation","GPU-based","GPU-instancing","real-time","large scale scenarios","optimization","Rendering"],"title":"GPU-Based Rendering and Collision Simulation of Ground Vegetation in Large-Scace Virtual Scenarios"},{"categories":null,"contents":"","permalink":"/publications/procedural-editing-of-virtual-terrains-using-3d-bezier-curves/","tags":["Bezier Curves","procedural","terrain editing","tool programming","terrain carving","virtual terrains","optimization"],"title":"Procedural Editing of Virtual Terrains Using 3D Bezier Curves"},{"categories":null,"contents":"","permalink":"/publications/gpu-based-real-time-procedural-distribution-of-vegetation-on-large-scale-virtual-terrains/","tags":["Procedural Generation","Large scale scenarios","Virtual terrains","Real-time","GPU-based","optimization","Rendering"],"title":"GPU-Based Real-Time Procedural Distribution of Vegetation On Large-Scale Virtual Terrains"},{"categories":null,"contents":"","permalink":"/publications/uma-framework-para-distribuicao-gerenciamento-e-renderizacao-de-vegetacao-em-cenarios-virtuais-massivos-em-tempo-real/","tags":["Vegetation","procedural generations","Virtual Landscapes","Collision Simulation","GPU-Based","Large Scale scenarios","Real-Time","optimization","Rendering"],"title":"Uma Framework para distribuicao, gerenciamento e renderizacao de vegetacao em cenarios virtuais massivos em tempo real"},{"categories":null,"contents":"","permalink":"/publications/aplica%C3%A7%C3%A3o-de-t%C3%A9cnicas-procedurais-em-gpu-para-renderiza%C3%A7%C3%A3o-de-nuvens-volum%C3%A9tricas-em-tempo-real/","tags":["Procedural Clouds","Procedural generation","Ray Marching","Noise","Rendering"],"title":"Aplicacao de tecnicas procedurais em GPU para renderizacao de nuvens volumetricas em tempo real"},{"categories":null,"contents":"","permalink":"/publications/using-active-mediators-and-passive-extractors-inside-materialized-data-integration-systems/","tags":["Data extraction","data integration","data analysis automation"],"title":"Using Active Mediators and Passive Extractors Inside Materialized Data Integration Systems"},{"categories":null,"contents":" .center { display: block; margin-left: auto; margin-right: auto; max-width: 95%; } .row { display: flex; margin-left: auto; margin-right: auto; max-width: 95%; } .column { flex: 33.33%; padding: 2px; } video { width: 100%; height: auto; display: block; margin-left: auto; margin-right: auto; } #player-overlay { position: absolute; display: none; top: 0; left: 0; width: 100%; height: 100%; background-color: #000; z-index: 999; }  I prototyped two solutions for glass-breaking simulation.\nApproach 1 - Simulating Broken Glass from Projectiles\n Support for various patterns defined by artists; Flexible control over color, alpha tests, opacity, and scale randomness; Accurate projection of patterns onto non-static glass surfaces; The entire process is executed in screen-space using my custom projection mathematics.  \nApproach 2 - Simulating Broken Glass from Larger Objects\n Designed for breaking larger glass areas, such as those impacted by stones or sizable objects; Control over the size of the impact; Addresses the challenge of \u0026ldquo;Floating Zones\u0026rdquo;, referring to glass areas without contact with other surfaces (image below); GPU optimizations to update the required masks;  \nResults\nHere are short videos demonstrating the obtained results. It is important to highlight that it is essential to use a particle system for more immersive results.\n       ","permalink":"/projects/creations/breaking-glass/","tags":["Compute Shaders","Computer Graphics","Procedural","Shader","Real-time","Collision Simulation"],"title":"Break Glass"},{"categories":null,"contents":" * { box-sizing: border-box; } .img-container-x4 { float: left; width: 25%; padding: 1px; } .note { font-size: 13px; } .img-container-x4:hover img { float: left; width: 150%; padding: 1px; } .clearfix::after { content: \"\"; clear: both; display: table; }  I proposed and improved a feature that compares millions of human faces to use GPU power, reducing the processing time from days to minutes.\n\nDescription\nI recently made a breakthrough in the field of face recognition technology by migrating a feature that was previously limited to CPU to GPU processing. This refactoring has drastically reduced the time for comparing N faces with another set of N faces, shrinking the time from days to a few minutes.\nTo better understand the magnitude of this achievement, imagine comparing 10 million faces with another 10 million faces (a small dataset compared to our clients). This would require an astonishing 100 trillion comparisons! Before the refactoring, this task would have taken days to process on a CPU. Now, it can be completed in a few minutes.\nThis achievement was made possible through the utilization of cutting-edge technologies and optimization techniques, including:\n Cuda/Parallelism; SIMD (Single Instruction Multiple Data); Quantization; DOD (Data-Oriented Design).  \nNote: I am not responsible for creating anything related to AIs. I simply utilize my expertise in optimizations and take advantage of hardware capabilities. ","permalink":"/projects/creations/facematch/","tags":["Cuda","Optimization","GPU","SIMD","Quantization","DDO (Data Oriented Design)","C++"],"title":"Facial Recognition Optimization"},{"categories":null,"contents":" * { box-sizing: border-box; } .video-container { float: left; width: 50%; padding: 2px; } .clearfix::after { content: \"\"; clear: both; display: table; } .center { display: block; margin-left: auto; margin-right: auto; max-width: 50%; } video { width: 95%; height: auto; display: block; margin-left: auto; margin-right: auto; }  The client\u0026rsquo;s new requirements added numerous game entities, drastically impacting the game\u0026rsquo;s performance. In this context, for four months, I focused on investigating and fixing performance bottlenecks, improving the game\u0026rsquo;s scalability, and bringing its performance to acceptable levels again.\nDuring this period, I worked on and refactored various game components, including:\n Controllers; Sounds system; Animations; Render; Physics; Particle systems; Introduced DOTS in some components.  These improvements not only restored the game\u0026rsquo;s performance but also improved the scalability, reducing the impacts of future requirements.\n  The video below shows the performance comparisons of before and after improvements, having the movement of four ASTROS batteries (composed of ~14 military vehicles each) as a test case. The camera focuses on a particular battery, but another three are also being simulated.\n ","permalink":"/projects/creations/game-optimizations/","tags":["Optimization","Performance","Memory Management","Scalability","Data Structure","DOTS"],"title":"Game Optimizations"},{"categories":null,"contents":" * { box-sizing: border-box; } .img-container { float: left; width: 25%; padding: 2px; } .video-container { float: left; width: 50%; padding: 2px; } .clearfix::after { content: \"\"; clear: both; display: table; } .center { display: block; margin-left: auto; margin-right: auto; max-width: 50%; }  I developed a solution for the real-time long-lasting grass deformation according to the vehicles' and characters' movements.\nIn real-world battlefields, vehicle tracks are dangerous as they can reveal position to enemies. Thus, a feature to simulate this behavior is widely imperative for military training. However, in large-scale terrain, keeping the grass deformed for hours or even days of simulation can be incredibly memory-intensive. For this reason, this functionality was denied many times to the client and also drove my determination to seek a solution.\n Long-lasting grass deformation, supporting hours or even days of simulation; Control restoration times based on the type of collider, such as vehicles or humans; Scalable for numerous colliders; Deformation computed on GPU side using compute-shaders; Advanced data structures and compression techniques for efficient memory management, resulting in memory savings of over 90%; Seamless terrain integration, enhancing the visuals for fly-cameras; GPU-instancing to optimize the grass rendering;   Demonstration of multiple colliders interacting with plants (on the left) and different restoration times for grass interacting with the tank\u0026rsquo;s base or tracks (on the right).\n    \nSolution\nThe approach involves dividing the scenarios into smaller sections using a quadtree and implementing a vector field to determine the direction of plants for each node. The vector fields are updated accordingly to objects move through the scenario. For performance purposes, complex 3D objects are approximated by planes and spheres (image below). When the colliders interact with the grass, vector fields are updated and the cost is assigned to restore each vector, allowing the simulation of different behaviors. For example, if a human disrupts the grass, it can quickly regain its shape, whereas if a vehicle damages the vector field, it may take longer to restore it (you can see both of these behaviors in the accompanying video).\n\nIn the vertex shader, grasses and small shrubs sample the vector fields to identify their orientation based on the interpolation of the closest vectors. Below we have a color debug showing the direction of the vectors in the vector fields after moving a vehicle. Also, we have the result after rendering, showing the vehicle\u0026rsquo;s path.\n     \nIn the fragment shader stage to texture the terrain the vector fields are sampled for a slight color attenuation. Here, the sample in the vector field is used to make a cross product with the up_vector, resulting in a scalar value used to attenuate the terrain fragment\u0026rsquo;s color. It is interesting to highlight deformation in far regions without 3D grams. The two images on the left show the before and after vehicles' movement, highlighting the path by terrain\u0026rsquo;s texturing. The two images on the right show the influence of the trails projected on terrain to improve the blend of deformed grams and the terrain.\n     \nCompute-shaders were used to deform and restore the vector fields. LOD and GPU-instancing techniques were used to optimize plants' rendering. Vector fields are kept in a TextureAtlas to reuse memory, preventing allocations and memory fragmentation. Vector fields far way from the camera without dynamic objects close are compressed (using the LZ4 algorithm) to save memory. When needed, the vector fields are decompressed. Then, it is possible to maintain deformations for long periods with a low storage cost. Due to the minimal variation in vector field data, compression rates can be extremely high, resulting in memory savings of 90-95%.\n\nMore details can be seen here.\n","permalink":"/projects/creations/grass-and-terrain-deformation/","tags":["Memory Management","Compute shaders","Computer graphics","Shader","Data compression","Real-time","GPU-Based","Collision Simulation","Large-Scale scenario"],"title":"Grass Deformation"},{"categories":null,"contents":" * { box-sizing: border-box; } .img-container-x4 { float: left; width: 33%; padding: 1px; } .img-container-x4:hover img { float: left; width: 150%; padding: 1px; } .clearfix::after { content: \"\"; clear: both; display: table; }  Some results that I achieved using Blender, purely for educational purposes and to explore the tool\u0026rsquo;s potentials.\n\nM107\n    \nFlamethrower\n    \nMossberg\n    \nBBS-CM (High-poly)\n    \nGranade Launcher L79\n    \nBlaser R93\n   ","permalink":"/projects/creations/blender_modeling/","tags":["Blender","Modeling"],"title":"Modeling (Blender)"},{"categories":null,"contents":" * { box-sizing: border-box; } .img-container-x4 { float: left; width: 25%; padding: 1px; } .img-container-x4:hover img { float: left; width: 150%; padding: 1px; } .clearfix::after { content: \"\"; clear: both; display: table; }  Some results that I achieved using Houdini, purely for educational purposes and to explore the tool\u0026rsquo;s potentials.\n\nDusty terrain\n   \nFiery terrain\n  \nIrregular Ground stone\n ","permalink":"/projects/creations/houdini/","tags":["Substance Designer","Procedural Material"],"title":"Procedural 3D - Houdini"},{"categories":null,"contents":" * { box-sizing: border-box; } .img-container-x3 { float: left; width: 33%; padding: 1px; } .img-container-x1 { float: left; width: 49.5%; padding: 1px; } .img-container-x3:hover img { float: left; width: 150%; padding: 1px; } .img-container-x1:hover img { float: left; width: 150%; padding: 1px; } .clearfix::after { content: \"\"; clear: both; display: table; } .center { display: block; margin-left: auto; margin-right: auto; max-width: 85%; } .left-arrow-container { display: flex; align-items: center; justify-content: center; } .left-arrow { font-size: 24px; /* Adjust the size as needed */ }  I modeled a set of 3D bridges parts to compose a procedural bridge system.\n\nDescription\nI was involved in the system design and also modeled many bridge parts to compose a flexible Procedural Bridge System, capable of instantiating many bridges during loading time based on a set of initial and final points. My experience in programming and art proved to be valuable during the system design process, as it facilitated effective communication with my colleague responsible for coding. It also helped me to have a straightforward understanding of the steps he followed, which enabled me to model a set of accurate 3D bridges' parts that met his needs.\nBelow are four variations of bridges created based on existing examples within the Brazilian Army\u0026rsquo;s training fields in Goias/Brazil. We also have a comparison of changing the starting and ending points of one bridge, resulting in a longer bridge.\n         ","permalink":"/projects/creations/procedural-bridge-system/","tags":["Procedural","Modeling","UV unwrapping","Texturing"],"title":"Procedural Bridge System"},{"categories":null,"contents":" * { box-sizing: border-box; } .img-container-x4 { float: left; width: 25%; padding: 1px; } .img-container-x4:hover img { float: left; width: 150%; padding: 1px; } .clearfix::after { content: \"\"; clear: both; display: table; }  Some materials created with Substance Designer, primarily focused on learning the tool and also furthering my proficiency in procedural generation.\n\nAged Wood\n    \nMuddy with Trails\n    \nBark with Knots and moss\n    \nGround with Roots\n   ","permalink":"/projects/creations/substance_designer/","tags":["Substance Designer","Procedural Material"],"title":"Procedural Materials - Substance Designer"},{"categories":null,"contents":" * { box-sizing: border-box; } .img-container-x4 { float: left; width: 33%; padding: 1px; } .img-container-x4:hover img { float: left; width: 150%; padding: 1px; } .clearfix::after { content: \"\"; clear: both; display: table; } video { width: 70%; height: auto; display: block; margin-left: auto; margin-right: auto; }  I designed and developed a robust GPU-based vegetation system capable of handling thousands of plants efficiently in real-time.\n\nDescription\nIn small environments, the decoration is typically handled by artists or designers, enabling precise control and aesthetically pleasing results. Nevertheless, as the environment size grows, this approach becomes increasingly time-consuming and impractical. To address this challenge, procedural solutions are often used, offering significant time savings. In this context, I have developed a solution that implements best practices for efficiently distributing and rendering thousands of trees and small plants in real-time.\nFeatures:\n Procedural distribution: enabling plant placement and rendering in expansive scenarios exceeding 500x500km.  On-demand distribution just for visible areas; Deterministic plant redistribution for revisited areas;   Global control for plant species: You can control the distribution of different plant species (oak tree, palm tree, cactus, grass) based on:  Pre-defined polygons that describe forests; Placement near pre-defined lines like rivers; Exclusion of undesirable zones such as lakes and roads; Integration with manually designated placement zones within the pre-defined polygons; Heightmap characteristics (slope, height) through adaptability curves; Weather-related aspects (temperature, precipitation) using 2D input maps and adaptability curves;   LOD management and efficient rendering:  Individual LOD settings for each plant species to finely balance performance and visual quality; LOD selection for each plant is efficiently computed compute-shaders; Frustum culling for each plant is efficiently computed using compute shaders; Maximum view range defined for each species; GPU-instancing to reduces drastically the draw-calls (just 2*№_LODs draw-calls per specie);   Resource optimization:  Minimum CPU usage; Minimum RAM\u0026lt;-\u0026gt;VRAM data transfers; No continuous memory allocation;   Physics integration:  On-demand spawn of colliders for plants close to characters;    Little or not explored:\n Advanced lighting (subsurface scattering, GI, lighting bake); Advanced quality assets (used trees have 8-12K triangles in LOD0); Cross-fading for smooth LOD transitions.    Below we have some visual results.\n               Below it is possible to see the rendering of several plants in a long-distance view.\n     Below we have a short performance demonstration (Ryzen 3, GTX 1070, 16GB RAM, Full HD). In this demonstration, the terrain spans 80km on its larger side, while trees are visible at distances of up to 8km.\n     High-level details about the solution\nThe solution distributes vegetation procedurally on-demand in regions delimited by quadtree\u0026rsquo;s nodes. Nodes outside the camera\u0026rsquo;s frustum are destroyed as well as the plants of it. The plants remain in memory as long as the nodes are visible. If needed, the plants are deterministically redistributed as the nodes are recreated.\nFor distribution, the solution considers the influence of different aspects, such as terrain (e.g., slope and moisture), environmental (e.g., forest, lakes, and rivers zones), and human (e.g., villages and roads) aspects. These aspects must be normalizable values into [0, 1] for any scenario\u0026rsquo;s portion, encoded in 2D maps and/or vector data. Any other information discretized into [0-1] can be integrated.\nPlants are grouped using the concept of presets, characterized by a set of plants that share the same needs, considering distribution aspects and other fields used to exist in some scenario\u0026rsquo;s position. For example, a presets of plants that exist in 1) forest zones, 2) not in steep regions, and 3) in higher altitude regions. The system analyzes many presets in real-time and defines which preset best fits a particular scenario\u0026rsquo;s position. After determining the best presets for the analyzed position, any plant from that can be randomly selected to be placed.\nFor efficiency purposes, plants are analyzed frame-by-frame to establish a LOD to balance performance and visuals. The LOD is defined from the distance of each plant to the camera \u0026ndash;artists have control over it. Also, plants outside the frustum are discarded before rendering. Finally, the plant data is organized properly for use of GPU-instancing to speed up rendering.\nEven though vegetation processing takes place mainly on the GPU, other processes on the CPU can still interact with plants. Objects can deform grass and small bushes, keeping them bent for long periods with acceptable memory demands (see more details here). Plants' colliders are instanced on-demand based on physics objects' positions to allow collisions with large and medium-sized trees using the physics engine.\nAs a result, A scalable GPU-based solution was developed to deal with any type of plant (e.g., trees, bushes, and grass) in real-time. It is possible to save different configurations \u0026ndash;equivalent to biomes\u0026ndash; and use them in different scenarios. Artists can intuitively edit these configurations to get different layouts for plant placement, previewing the result immediately. In addition, artists can edit LOD parameters to get a trade-off between visuals and performance. The solution also extracts maximum parallelism and processing from modern GPUs.\n","permalink":"/projects/creations/procedural-vegetation-system/","tags":["Procedural","Compute shaders","Computer graphics","Shader","Real-time","GPU-Based","GPU-Instancing","Large-Scale scenario"],"title":"Procedural Vegetation System"},{"categories":null,"contents":" * { box-sizing: border-box; } .img-container-x4 { float: left; width: 33%; padding: 1px; } .img-container-x4:hover img { float: left; width: 150%; padding: 1px; } .clearfix::after { content: \"\"; clear: both; display: table; }  Some results that I achieved using ZBrush, purely for educational purposes and to explore the tool\u0026rsquo;s potentials.\nOctopus\n    \nPumpkin Head\n    \nSword\n   ","permalink":"/projects/creations/zbrush_sculpting/","tags":["ZBrush","Sculpting"],"title":"Sculpting (ZBrush)"},{"categories":null,"contents":"This file exists solely to respond to /search URL with the related search layout template.\nNo content shown here is rendered, all content is based in the template layouts/page/search.html\nSetting a very low sitemap priority will tell search engines this is not important content.\nThis implementation uses Fusejs, jquery and mark.js\nInitial setup Search depends on additional output content type of JSON in config.toml\n[outputs] home = [\u0026#34;HTML\u0026#34;, \u0026#34;JSON\u0026#34;] Searching additional fileds To search additional fields defined in front matter, you must add it in 2 places.\nEdit layouts/_default/index.JSON This exposes the values in /index.json i.e. add category\n... \u0026#34;contents\u0026#34;:{{ .Content | plainify | jsonify }} {{ if .Params.tags }}, \u0026#34;tags\u0026#34;:{{ .Params.tags | jsonify }}{{end}}, \u0026#34;categories\u0026#34; : {{ .Params.categories | jsonify }}, ... Edit fuse.js options to Search static/js/search.js\nkeys: [ \u0026#34;title\u0026#34;, \u0026#34;contents\u0026#34;, \u0026#34;tags\u0026#34;, \u0026#34;categories\u0026#34; ] ","permalink":"/search/","tags":null,"title":"Search Results"},{"categories":null,"contents":" video { width: 70%; height: auto; display: block; margin-left: auto; margin-right: auto; } #player-overlay { position: absolute; display: none; top: 0; left: 0; width: 100%; height: 100%; background-color: #000 z-index:999; }  I developed a dynamic data structure to optimize access to mass datasets of polygons and polylines. This structure accelerated terrain rendering by an average of +80%.\nIn military simulations, mass GIS (Geographic Information System) datasets are a common resource used to represent features like forests, lakes, roads, and rivers through polygons and polylines. These datasets play a vital role in defining the materials in large terrains (exceeding 100x100km) where traditional material baking is storage-intensive and can be impractical. This leads to the need for real-time texture rendering. Despite the use of acceleration data structures (e.g., hash, BVH, and quadtree) to reduce data access costs in terrain rendering, there may still be limitations to their effectiveness.\nTo address the issue of costly access to bulk GIS datasets for terrain rendering, I developed an intermediate data structure between the terrain shaders and acceleration data structures. This structure significantly streamlines material selection and minimizes access to the acceleration data structures, thereby enhancing performance.\nThe structure in summary:\n On-demand, we select visible areas and set up the structure using compute shaders; Each visible area is represented by a 2D texture. Each pixel covers 10x10cm, requires 1 byte, and encodes until 8 features (e.g., forests and rivers); The encoded data not only defines the feature(s) but also determines whether access to the acceleration data structure is necessary for more precise data; It supports multilayers, allowing pixels from distant areas to cover larger areas, thereby saving memory; The structure is computed and released on-demand, optimizing memory consumption; Memory is reused to prevent memory allocation spikes and fragmentation; No data transfer between RAM and VRAM after the game setup, enhancing efficiency.  \nCheck the demonstration in a terrain with 80x35km.\n Red fragments indicate access to data structures (costly); Blue fragments represent my data structure (much cheaper) in use to define the fragment material; Observe the toggles and the performance changes.   ","permalink":"/projects/creations/vector-feature-map/","tags":["Memory Management","Compute shaders","Computer graphics","Shaders","Real-time","GPU-Based","Large-Scale Scenario"],"title":"Vector Data Access Optimization"},{"categories":null,"contents":" * { box-sizing: border-box; } .img-container-x4 { float: left; width: 49%; padding: 1px; } .img-container-x4:hover img { float: left; width: 150%; padding: 1px; } .clearfix::after { content: \"\"; clear: both; display: table; }   Full creation of low-poly vehicles constituting the Brazilian Astros II (Artillery SaTuration ROcket System) battery.\n 3D Modeling: Blender UV Unwrapping: Blender Level of Details (LOD): Blender (Decimate tool) Texturing: Substance Painter Bake: Blender (Normal and Ambient Occlusion map) Real-time dynamic animation: Code  Antennas (Blend shapes and C#) Mechanical arm (Inverse kinematics with C#) Leveling pistons (C#) Tailgate (C#)                 ","permalink":"/projects/creations/astros_vehicles/","tags":["Substance Painter","Blender","UV unwrapping","Texturing","Programming","Animation"],"title":"Vehicles creation - 3D artist"},{"categories":null,"contents":" * { box-sizing: border-box; } .img-container-x4 { float: left; width: 49%; padding: 1px; } .img-container-x4:hover img { float: left; width: 150%; padding: 1px; } .clearfix::after { content: \"\"; clear: both; display: table; } video { width: 70%; height: auto; display: block; margin-left: auto; margin-right: auto; }  I created a volumetric procedural clouds using the ray-marching technique.\n\nDescription\nI developed a real-time volumetric cloud rendering solution using ray-marching technique for my BSc project inspired by the Guerrilla team\u0026rsquo;s proposal. While the visual results didn\u0026rsquo;t match their achievements, it\u0026rsquo;s a project I take great pride in, as it marked the beginning of my studies into shaders and proceduralism.\nSettings:\n Density; Fluffly aspect; Height; Sparseness; Wind animation; Amount of rays casted for performance control.  Not implemented:\n Shadow projection over other objects; Optimizations proposed by Guerrilla\u0026rsquo;s team.         \nRay-marching technique overview\nThe ray-marching solution involves casting rays through a volume and incrementally advancing them until a stopping condition is reached. The stop condition typically is when the ray has lost all its energy. The volume is described by various noises (e.g., Perlin and Worley noise), which when combined effectively describe clouds. A series of parameters define how to interpret this volume, allowing for the creation of clouds with different sizes and sparseness, as well as with more or less \u0026ldquo;fluffy\u0026rdquo; details. Other parameters also allow for the description of how light rays behave, generating denser clouds with different visual characteristics.\n\nMore details can be seen here.\n","permalink":"/projects/creations/clouds/","tags":["Shader","Computer Graphics","Procedural","Real-time"],"title":"Volumetric Procedural Clouds"},{"categories":null,"contents":" .center { display: block; margin-left: auto; margin-right: auto; max-width: 95%; } .row { display: flex; margin-left: auto; margin-right: auto; max-width: 95%; } .column { flex: 25%; padding: 2px; }  I designed and guided an intern in developing a tool to generate polygonal meshes from mass polygons and polylines datasets, representing rivers (lines) and lakes (polygons) in large-scale scenarios.\n\nDescription\nThe purpose of this tool is to replace an outdated solution that struggled with scalability issues at entity junctions, such as river-river (line-line, on the left), rivers-rivers (lines-lines, in the middle), and river-lake (line-polygon, on the right). The previous solution employed a geometric approach that required treating each case of junctions, resulting in a system that was difficult to expand and maintain.\n\nTo get a simple and scalable solution for different types and amounts of entities' junctions, we developed a raster-based solution. The process starts by filling a texture (Buffering step, image below), where each pixel represents a small portion of the scenario and is set as a region Inside, Outside, or Border of a water body (river and/or lake). Next, the texture is traversed to extract the pixels defined as a Border (Vectorization step), which will be Simplified and Triangulated in the sequence. It results in a single mesh containing rivers and lakes from the region considered (whole scenario or small blocks for performance reasons). For performance purposes, compute-shader and acceleration structures were used.\n\nMore details can be seen here.\n","permalink":"/projects/creations/water-bodies-meshes-generation/","tags":["Compute Shaders","Computer Graphics","Data Structure","GIS Data","GPU-Based","Large-Scale scenario","Tools","Tutoring"],"title":"Water planes mesh generation"}]